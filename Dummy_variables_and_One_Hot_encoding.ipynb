{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIcDzvN/rd9ycS9PyYDqAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushka091922/Deep_Learning_tutorial/blob/main/Dummy_variables_and_One_Hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6ykbjKqb-S0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy variables and One Hot encoding"
      ],
      "metadata": {
        "id": "A8MWbC6Ob_o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the main issue here is how handle the text data? because the machine learning model is good in handling the numeric data\n",
        "# the one way to deal with this is the integer encoding or the label encoding where we use the name of the town and thne assign numbers to it . but the next issue here with this\n",
        "# approach is that the machine learning model may think that the town with some number is smaller or gretaer than the other town or may be more or less importnat than the other town\n",
        "# hence as  a soluiton to all these we make use of the one hot encoding\n",
        "# to join two data frames we make use of the concat\n",
        "# when one variable can be derived from the rest of the variables then these varibales are set to have multicolinear . and when the varibales are multicolinear, that creats the problem\n",
        "# of the dummy variable traps . (learn more about the dummy varibales and why we have to drop one of the summy variable\n",
        "\n"
      ],
      "metadata": {
        "id": "8UeKSHaRcFC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping the Dummy variable columns"
      ],
      "metadata": {
        "id": "-67yb8difCo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we have dropped the price colums in the given example\n"
      ],
      "metadata": {
        "id": "VwFhhISQfJSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy Variable Trap"
      ],
      "metadata": {
        "id": "p1JbQ8A1ftUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# When you can derive one variable from other variables, they are known to be multi-colinear. Here if you know values of california and georgia then you can easily infer value of new jersey state, i.e. california=0 and georgia=0. There for these state variables are called to be multi-colinear. In this situation linear regression won't work as expected. Hence you need to drop one column.\n",
        "\n",
        "# NOTE: sklearn library takes care of dummy variable trap hence even if you don't drop one of the state columns it is going to work, however we should make a habit of taking care of dummy variable trap ourselves just in case library that you are using is not handling this for you\n",
        "\n"
      ],
      "metadata": {
        "id": "S0F03ezyfrAF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}